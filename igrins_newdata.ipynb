{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the cross matched superlog\n",
    "xmatch_superlog = pd.read_csv('./xmatch_log.csv')\n",
    "igrins_old = pd.read_csv('./standard_table_v2.txt', index_col=0)\n",
    "\n",
    "standard_names = igrins_old['Name']\n",
    "# standard_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hd286178.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hqtau.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hbc427.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  v830tau.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hubble4.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anon1.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xest09.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lkca14.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  haro6.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  jh108.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hbc359.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  jh433.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mho7.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xray5.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
      "C:\\Users\\Savio\\AppData\\Local\\Temp\\ipykernel_28688\\3931357462.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mho8.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "hd286178 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('286178')]\n",
    "hqtau = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('HQ Tau')]\n",
    "hbc427 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('HBC 427')]\n",
    "v830tau = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('V830')]\n",
    "hubble4 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('V1023')]\n",
    "anon1 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('V1096')]\n",
    "xest09 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('09-042')]\n",
    "lkca14 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('HBC 417')]\n",
    "haro6 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('Haro 6-13')]\n",
    "jh108 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('JH 108')]\n",
    "hbc359 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('HBC 359')]\n",
    "jh433 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('JH 433')]\n",
    "mho7 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('3314352530621527296')] # Gaia DR3 ID\n",
    "xray5 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('V410 X-ray 5')]\n",
    "mho8 = xmatch_superlog[xmatch_superlog['IDS'].astype('str').str.contains('MHO 8')]\n",
    "\n",
    "hd286178.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "hqtau.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "hbc427.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "v830tau.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "hubble4.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "anon1.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "xest09.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "lkca14.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "haro6.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "jh108.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "hbc359.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "jh433.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "mho7.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "xray5.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "mho8.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8698    74.363922\n",
      "Name: SNRK_pix, dtype: float64 875    71.505295\n",
      "Name: SNRK_pix, dtype: float64 675    61.629185\n",
      "Name: SNRK_pix, dtype: float64 473    50.797092\n",
      "Name: SNRK_pix, dtype: float64 337    163.912628\n",
      "Name: SNRK_pix, dtype: float64 300    153.189178\n",
      "Name: SNRK_pix, dtype: float64 499    55.310417\n",
      "Name: SNRK_pix, dtype: float64 502    37.395519\n",
      "Name: SNRK_pix, dtype: float64 441    81.427101\n",
      "Name: SNRK_pix, dtype: float64 4805    33.738834\n",
      "Name: SNRK_pix, dtype: float64 5597    35.849026\n",
      "Name: SNRK_pix, dtype: float64 8760    57.330921\n",
      "Name: SNRK_pix, dtype: float64 1723    18.440859\n",
      "Name: SNRK_pix, dtype: float64 2358    37.600754\n",
      "Name: SNRK_pix, dtype: float64 2398    22.307974\n",
      "Name: SNRK_pix, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(hd286178['SNRK_pix'],\n",
    "hqtau['SNRK_pix'],\n",
    "hbc427['SNRK_pix'],\n",
    "v830tau['SNRK_pix'],\n",
    "hubble4['SNRK_pix'],\n",
    "anon1['SNRK_pix'],\n",
    "xest09['SNRK_pix'],\n",
    "lkca14['SNRK_pix'],\n",
    "haro6['SNRK_pix'],\n",
    "jh108['SNRK_pix'],\n",
    "hbc359['SNRK_pix'],\n",
    "jh433['SNRK_pix'],\n",
    "mho7['SNRK_pix'],\n",
    "xray5['SNRK_pix'],\n",
    "mho8['SNRK_pix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the SIMBAD MAIN_ID column because the IGRINS log names may be different for the same object\n",
    "#again inplace = True permanently changes the dataframe\n",
    "# xmatch_superlog.drop_duplicates(subset = ['MAIN_ID'], keep = 'first', inplace = True)\n",
    "# xmatch_superlog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here iloc[0] grabs the first row of the dataframe as it is currently sorted regardless of the index value\n",
    "#(.loc[0] would select the row that is assigned the index value of 0)\n",
    "# xmatch_subset = xmatch_superlog[xmatch_superlog['SNRK_pix'] > 100]\n",
    "# xmatch_subset['IDS'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #it is important to convert the Series (subset['IDS']) into a string (.astype(str)) so that way we do not get any errors from NaN values\n",
    "# #note that this is a case sensitive search!\n",
    "# subset_hops = xmatch_subset[xmatch_subset['IDS'].astype('str').str.contains('HOPS')]\n",
    "# #we can see that the first object in this new dataframe has several identifiers with \"Tau\" in the name!\n",
    "# subset_hops['IDS'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subset_hops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#we can grab the filename of the file we want to download\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#here .split('.') splits the filename at the . so we can remove the .fits extension from the string\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[43msubset_hops\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILENAME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#this is the download URL for the zip file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m download_link \u001b[38;5;241m=\u001b[39m subset_hops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILE_URL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subset_hops' is not defined"
     ]
    }
   ],
   "source": [
    "#we can grab the filename of the file we want to download\n",
    "#here .split('.') splits the filename at the . so we can remove the .fits extension from the string\n",
    "file_name = subset_hops['FILENAME'].iloc[0].split('.')[0]\n",
    "\n",
    "#this is the download URL for the zip file\n",
    "download_link = subset_hops['FILE_URL'].iloc[0]\n",
    "\n",
    "#an example of a folder where you might want to put the file\n",
    "civil_date = subset_hops['CIVIL'].iloc[0]\n",
    "\n",
    "#using a session here means that requests can fetch the information faster since the connection is reused\n",
    "#this means you can download multiple files at once using the same session if you want!\n",
    "session = requests.Session()\n",
    "\n",
    "#the response holds lots of information gathered from the request (session.get())\n",
    "response = session.get(download_link)\n",
    "\n",
    "#we can check the status code to make sure our link was found successfully\n",
    "if response.status_code == 200:\n",
    "    #open a file with the corresponding filename to dump the file content into\n",
    "    #here we are putting the file into a folder with the corresponding civil date of the observation\n",
    "    #we are also adding the .tar.gz extension to the filename\n",
    "    with open(f\"./{file_name}.tar.gz\", 'wb') as f:\n",
    "        #write the file\n",
    "        f.write(response.content)\n",
    "        \n",
    "    #close the file--especially important when writing many files at once or trying to prevent against corrupted files\n",
    "    f.close()\n",
    "    #print that we downloaded the file\n",
    "    print(f\"Downloaded {file_name}.tar.gz to {civil_date} folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "downloads files into organized directories\n",
    "\n",
    "input:\n",
    "    download_link: the Box download link for the file\n",
    "    file_number: the file number of the target\n",
    "    civil: the civil date of observation\n",
    "    name: the name of the object \n",
    "    session: a requests Session object \n",
    "'''\n",
    "def download_files(download_link, file_number, civil, name, session):\n",
    "    #get the url for the filename\n",
    "    response = session.get(download_link)\n",
    "    #make sure the file url was found\n",
    "    if response.status_code == 200:\n",
    "        #create the directory for the file, if it is already created there will not be an error\n",
    "        os.makedirs(os.path.dirname(f\"{civil}/{name}/\"), exist_ok=True)\n",
    "        #open the file\n",
    "        #creating a new name for our downloaded file with the civil date and the file number\n",
    "        with open(f\"{civil}/{name}/{civil}_{str(file_number).zfill(4)}.tar.gz\", 'wb') as f:\n",
    "            #write the information to the file\n",
    "            f.write(response.content)\n",
    "        #close the file\n",
    "        f.close()\n",
    "        #print that the file was downloaded\n",
    "        print(f\"Downloaded {civil}_{str(file_number).zfill(4)}.tar.gz to the {civil} folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the reference session\n",
    "session = requests.Session()\n",
    "\n",
    "#for the first five rows in the DataFrame\n",
    "for idx in subset_hops.index[0:5]:\n",
    "    #get the civil date of the target\n",
    "    civil = subset_hops['CIVIL'].loc[idx]\n",
    "    #get the name of the target\n",
    "    #here we join any spaces in the name of the target with an '_' and remove any '*' characters from the name\n",
    "    name = '_'.join(subset_hops['MAIN_ID'].loc[idx].replace('*', '').split())\n",
    "    #download the tar files\n",
    "    download_files(subset_hops['FILE_URL'].loc[idx], subset_hops['FILENUMBER'].loc[idx], civil, name, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(subset_hops)):\n",
    "#     civil = subset_hops['CIVIL'].iloc[i]\n",
    "\n",
    "#     file_number = subset_hops['FILENUMBER'].iloc[i]\n",
    "\n",
    "#     name = '_'.join(subset_hops['MAIN_ID'].iloc[i].replace('*', '').split())\n",
    "\n",
    "#     #the unpack_archive function automatically detects the compression format of the zip file\n",
    "#     #the second argument in the function is the name for the folder of the unzipped file, I am just using the same as the zipped file name\n",
    "#     shutil.unpack_archive(f'{civil}/{name}/{civil}_{str(file_number).zfill(4)}.tar.gz', f'{civil}/{name}/{civil}_{str(file_number).zfill(4)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_hops[['NAME','OBJNAME_super','SP_TYPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'C:/Users/Savio/Documents/IGRINS-Spectra/'\n",
    "# #example file name\n",
    "# #here we are adjusting the file name so it no longer has the .fits extension attached to it\n",
    "# file_name = (xmatch_superlog['FILENAME'].iloc[0]).split('.')[0]\n",
    "# #download link for file name\n",
    "# download_link = xmatch_superlog['FILE_URL'].iloc[0]\n",
    "\n",
    "# #start a requests session (allows you to download multiple files at once)\n",
    "# session = requests.Session()\n",
    "# #get information from the website (from the link we pass) by making a response object\n",
    "# response = session.get(download_link)\n",
    "\n",
    "# #we can check the status code to make sure our link was found succssfully\n",
    "# if response.status_code == 200:\n",
    "#   #open a file on your computer to dump the file content from \n",
    "#   with open(f\"{filepath+file_name}.tar.gz\", 'wb') as f:\n",
    "#     #write the file\n",
    "#     f.write(response.content)\n",
    "#   #close the file--very important when writing multiple files at once!\n",
    "#   f.close()\n",
    "#   #print where the file was downloaded to\n",
    "#   print(f\"Successfully downloaded {file_name} to {filepath+file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #the unpack_archive function automatically detects the compression format of the zip file\n",
    "# #the second argument for the function here is the directory name for the unzipped file, we are just using the file name\n",
    "# shutil.unpack_archive(f\"{filepath+file_name}.tar.gz\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muler_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
